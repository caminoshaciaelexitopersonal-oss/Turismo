import os
import httpx
import tiktoken
from dotenv import load_dotenv
from typing import List, Dict

from asgiref.sync import sync_to_async

# --- Importaciones de Django (se inicializan de forma segura) ---
try:
    from api.models import SiteConfiguration, UserLLMConfig
    from api.local_llm_service import invoke_local_llm
    DJANGO_AVAILABLE = True
except (ImportError, ModuleNotFoundError):
    DJANGO_AVAILABLE = False
    # Mocks para que el archivo no falle al importarse
    class UserLLMConfig: pass
    async def invoke_local_llm(prompt: str, model_name: str) -> str:
        return f"Error: Django no est谩 disponible. No se puede invocar a {model_name}."

load_dotenv()

# --- Configuraci贸n ---
GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"
GROQ_MODEL_NAME = "llama3-8b-8192"
DEEP_REASONING_KEYWORDS = ["analiza", "resume", "explica", "corrige", "eval煤a", "genera un reporte", "crea un plan"]
LOCAL_MODEL_MAP = {
    "PHI3_LOCAL": "phi3:mini",
    "PHI4_LOCAL": "phi-4"
}

def count_tokens(text: str) -> int:
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))
    except Exception as e:
        print(f"Advertencia: No se pudo usar tiktoken. Usando conteo de palabras. Error: {e}")
        return len(text.split())

def requires_deep_reasoning(prompt: str) -> bool:
    prompt_lower = prompt.lower()
    return any(keyword in prompt_lower for keyword in DEEP_REASONING_KEYWORDS)

async def invoke_groq_api(prompt: str, api_key: str, conversation_history: List[Dict[str, str]]) -> str:
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    messages = conversation_history + [{"role": "user", "content": prompt}]
    data = {"model": GROQ_MODEL_NAME, "messages": messages, "temperature": 0.7}

    try:
        async with httpx.AsyncClient() as client:
            response = await client.post(GROQ_API_URL, headers=headers, json=data, timeout=90)
            response.raise_for_status()
            return response.json()['choices'][0]['message']['content']
    except httpx.RequestError as e:
        return f"Error de conexi贸n a la API de Groq: {e}"
    except (KeyError, IndexError, TypeError) as e:
        return f"Respuesta inesperada de la API de Groq: {response.text}. Error: {e}"

async def route_llm_request(prompt: str, conversation_history: List[Dict[str, str]], user) -> str:
    if not DJANGO_AVAILABLE:
        return "Error: El entorno de Django no est谩 configurado."

    # 1. Obtener la configuraci贸n del usuario de forma as铆ncrona
    try:
        config, _ = await UserLLMConfig.objects.aget_or_create(user=user)
        if config.provider == 'GROQ' and config.api_key:
            print(f" [LLM Router] Usando Groq personalizado del usuario {user.username}.")
            return await invoke_groq_api(prompt, config.api_key, conversation_history)
        elif config.provider in LOCAL_MODEL_MAP:
            model_name = LOCAL_MODEL_MAP[config.provider]
            print(f"锔 [LLM Router] Usando modelo local {model_name} (configuraci贸n de usuario) para {user.username}.")
            return await invoke_local_llm(prompt, model_name=model_name)
    except Exception as e:
        print(f"Advertencia: No se pudo obtener la configuraci贸n LLM del usuario. Usando router del sistema. Error: {e}")

    # 2. Si no hay config personalizada o falla, usar el router h铆brido del sistema
    print("癸 [LLM Router] Usando router h铆brido del sistema.")

    try:
        site_config = await sync_to_async(SiteConfiguration.load)()
        token_threshold = site_config.llm_routing_token_threshold
        groq_api_key_global = os.getenv("GROQ_API_KEY")
    except Exception as e:
        print(f"Advertencia: No se pudo cargar SiteConfiguration. Usando valores por defecto. Error: {e}")
        token_threshold = 1500
        groq_api_key_global = None

    history_text = " ".join([msg["content"] for msg in conversation_history])
    total_tokens = count_tokens(history_text + prompt)
    is_complex_task = requires_deep_reasoning(prompt)

    use_groq = total_tokens > token_threshold or is_complex_task

    if use_groq:
        if groq_api_key_global:
            print(f"[LLM Router] Tarea compleja/larga ({total_tokens} tokens). Usando Groq global del sistema.")
            return await invoke_groq_api(prompt, groq_api_key_global, conversation_history)
        else:
            print(f"[LLM Router] Tarea compleja/larga ({total_tokens} tokens) pero sin Groq global. Fallback a Phi-3 local.")
            return await invoke_local_llm(prompt, model_name=LOCAL_MODEL_MAP["PHI3_LOCAL"])
    else:
        print(f"[LLM Router] Tarea simple ({total_tokens} tokens). Usando modelo local: Phi-3 Mini.")
        return await invoke_local_llm(prompt, model_name=LOCAL_MODEL_MAP["PHI3_LOCAL"])