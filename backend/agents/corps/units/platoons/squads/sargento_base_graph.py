from typing import TypedDict, Any, List, Annotated
import operator
from langchain_core.messages import BaseMessage, HumanMessage
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode
from langchain_openai import ChatOpenAI
# Potencialmente para otros proveedores:
# from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.tools import BaseTool
from api.models import CustomUser


class SargentoBaseState(TypedDict):
    """La pizarra t谩ctica estandarizada para todos los Sargentos."""
    teniente_order: str
    app_context: dict  # Se espera que contenga {'user': CustomUser}
    messages: Annotated[List[BaseMessage], operator.add]
    final_report: str
    error: str | None


class SargentoGraphBuilder:
    """
    Constructor estandarizado para agentes Sargento.
    Construye un grafo que utiliza el proveedor de IA y la clave de API
    espec铆ficos del usuario que realiza la solicitud.
    """

    def __init__(self, squad: List[BaseTool], squad_name: str):
        self.squad = squad
        self.squad_name = squad_name
        self.squad_executor = ToolNode(self.squad)

    async def get_sargento_brain(self, state: SargentoBaseState):
        """
        El cerebro del Sargento. Construye din谩micamente el LLM con las
        credenciales del usuario y decide la siguiente acci贸n de forma as铆ncrona.
        """
        print(f"---  SARGENTO ({self.squad_name}): Analizando orden para el usuario... ---")

        user = state['app_context'].get('user')
        if not isinstance(user, CustomUser) or not user.api_key or not user.ai_provider:
            raise ValueError("Contexto de aplicaci贸n inv谩lido o configuraci贸n de IA del usuario incompleta.")

        # --- L贸gica Multi-Inquilino ---
        api_key = user.api_key

        if user.ai_provider == CustomUser.AIProvider.OPENAI:
            model = ChatOpenAI(model="gpt-4o", temperature=0, api_key=api_key)
        # Se pueden a帽adir otros proveedores aqu铆
        else:
            # Fallback o error si el proveedor no es soportado en esta capa.
            # Por ahora, asumimos que el router ya ha filtrado y solo llega OpenAI o un proveedor v谩lido.
            # Para este ejemplo, usaremos OpenAI como un default si algo falla.
            print(f"Advertencia: Proveedor '{user.ai_provider}' no implementado en la capa del Sargento. Usando OpenAI como fallback.")
            model = ChatOpenAI(model="gpt-4o", temperature=0, api_key=api_key)

        # Se enlazan las herramientas de la escuadra al modelo espec铆fico del usuario.
        bound_model = model.bind_tools(self.squad)

        # Usamos 'ainvoke' para la llamada as铆ncrona
        return await bound_model.ainvoke(state["messages"])

    def route_action(self, state: SargentoBaseState):
        """Revisa la decisi贸n del cerebro y enruta al ejecutor o al informe final."""
        last_message = state["messages"][-1]
        if not hasattr(last_message, 'tool_calls') or not last_message.tool_calls:
            return "compile_report"
        return "squad_executor"

    def compile_report_node(self, state: SargentoBaseState) -> SargentoBaseState:
        """Compila el informe final para el Teniente a partir del historial de la misi贸n."""
        print(f"---  SARGENTO ({self.squad_name}): Misi贸n completada. Compilando reporte. ---")
        final_ai_message = state["messages"][-1].content
        executed_steps = [
            f"Acci贸n: {msg.name}, Resultado: {msg.content}"
            for msg in state["messages"] if msg.type == 'tool'
        ]

        if not executed_steps:
            report_body = "Misi贸n completada sin necesidad de acciones directas de la escuadra."
        else:
            report_body = "Resumen de acciones de la escuadra:\n- " + "\n- ".join(executed_steps)

        state["final_report"] = f"{final_ai_message}\n\n---\n**Bit谩cora de Ejecuci贸n:**\n{report_body}"
        return state

    def build_graph(self):
        """Construye y compila el grafo LangGraph para el Sargento."""
        workflow = StateGraph(SargentoBaseState)

        def mission_entry_node(state: SargentoBaseState):
            """Nodo de entrada que formatea la orden del Teniente como el primer mensaje."""
            return {"messages": [HumanMessage(content=state["teniente_order"])]}

        workflow.add_node("mission_entry", mission_entry_node)
        workflow.add_node("brain", self.get_sargento_brain)
        workflow.add_node("squad_executor", self.squad_executor)
        workflow.add_node("compile_report", self.compile_report_node)

        workflow.add_edge(START, "mission_entry")
        workflow.add_edge("mission_entry", "brain")
        workflow.add_conditional_edges("brain", self.route_action, {
            "squad_executor": "squad_executor",
            "compile_report": "compile_report"
        })
        workflow.add_edge("squad_executor", "brain")
        workflow.add_edge("compile_report", END)

        return workflow.compile()


def get_sargento_base_graph():
    """
    Funci贸n de compilaci贸n para el Sargento Base.
    Utiliza un conjunto de herramientas vac铆o, ya que es una plantilla.
    """
    sargento_builder = SargentoGraphBuilder(squad=[], squad_name="Base")
    return sargento_builder.build_graph()